{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FBh_lRSgYNLo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mlflow\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FRs21_URYN_A"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import subprocess\n",
        "from pyngrok import ngrok, conf\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIZdVDyCYRjn",
        "outputId": "418b2133-4ff9-4810-cb93-fd5d141c0632"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['mlflow', 'ui', '--backend-store-uri', 'sqli...>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
        "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnygshxcYZCl",
        "outputId": "b68b3ac3-b1e1-43cc-cec5-f18a44ec6433"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/1', creation_time=1715896345845, experiment_id='1', last_update_time=1715896345845, lifecycle_stage='active', name='duration-prediction-experiment', tags={}>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "# mlflow will create an experiment if it doesn't exist\n",
        "mlflow.set_experiment(\"duration-prediction-experiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqNtqjd6Yabs",
        "outputId": "a3c11e2a-a808-4c29-8310-de4b9d371ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n",
            "··········\n",
            " * ngrok tunnel \"https://7112-35-202-246-121.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "port=5000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f' * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCcxVatxYcuq",
        "outputId": "6f8fcff5-f807-4c3b-d84c-e0dbf2ed19a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/2', creation_time=1715896355333, experiment_id='2', last_update_time=1715896355333, lifecycle_stage='active', name='BDL Project', tags={}>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlflow.set_experiment(\"BDL Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpl_rX-5Yjbe",
        "outputId": "c2c6f27e-0bbc-44f6-88d6-14d32d5596d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=fb128eb950c4c6b1448e0aa9764992af7027fc43f34af12d4976c0f834fd69ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI8pZx0zYlML",
        "outputId": "b20a9bd7-977e-4d05-88a4-cbfc7380526c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch import nn,optim\n",
        "import os\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cMHhqvx4YqCd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_checkpoint(checkpoint,model,optimizer,lr):\n",
        "    print(\"Loading checkpoint...\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "def save_checkpoint(state,filename='checkpt.pth.tar'):\n",
        "    print(\"Saving checkpoint...\")\n",
        "    torch.save(state,filename)\n",
        "\n",
        "# def get_submission(loader, dataset, model):\n",
        "#     model.eval()\n",
        "#     predictions = pd.DataFrame()\n",
        "#     image_id = 1\n",
        "\n",
        "#     for image, label in tqdm(loader):\n",
        "#         image = image.to(myconfig.DEVICE)\n",
        "#         preds = torch.clip(model(image).squeeze(0), 0.0, 96.0)\n",
        "#         predictions = pd.concat([predictions,pd.Series(preds.detach().numpy()).to_frame().T],ignore_index=True)\n",
        "\n",
        "#         image_id += 1\n",
        "\n",
        "#     predictions.to_csv(\"submission.csv\", index=False)\n",
        "#     model.train()\n",
        "\n",
        "def get_rmse(loader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    num_examples = 0\n",
        "    losses = []\n",
        "    for batch_idx, (data, targets) in enumerate(loader):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        scores = model(data)\n",
        "        loss = loss_fn(scores[targets != -1], targets[targets != -1])\n",
        "        num_examples += scores[targets != -1].shape[0]\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Loss on val: {(sum(losses)/num_examples)**0.5}\")\n",
        "    return (sum(losses)/num_examples)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VJJfcR5hYrWW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class FacialKeypointDataset(Dataset):\n",
        "    def __init__(self, csv_file, train=True, transform=None):\n",
        "        super().__init__()\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.category_names = ['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x', 'right_eye_center_y', 'left_eye_inner_corner_x', 'left_eye_inner_corner_y', 'left_eye_outer_corner_x', 'left_eye_outer_corner_y', 'right_eye_inner_corner_x', 'right_eye_inner_corner_y', 'right_eye_outer_corner_x', 'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y', 'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y', 'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x', 'mouth_center_top_lip_y', 'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y']\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            image = np.array(self.data.iloc[index, 30].split()).astype(np.float32)\n",
        "            labels = np.array(self.data.iloc[index, :30].tolist())\n",
        "            labels[np.isnan(labels)] = -1\n",
        "        else:\n",
        "            image = np.array(self.data.iloc[index, 0].split()).astype(np.float32)\n",
        "            labels = np.zeros(30)\n",
        "\n",
        "        ignore_indices = labels == -1\n",
        "        labels = labels.reshape(15, 2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = np.repeat(image.reshape(96, 96, 1), 3, 2).astype(np.uint8)\n",
        "            augmentations = self.transform(image=image, keypoints=labels)\n",
        "            image = augmentations[\"image\"]\n",
        "            labels = augmentations[\"keypoints\"]\n",
        "\n",
        "        labels = np.array(labels).reshape(-1)\n",
        "        labels[ignore_indices] = -1\n",
        "\n",
        "        return image, labels.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E7cSV6TXYss8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 8e-6\n",
        "WEIGHT_DECAY = 1e-3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "NUM_WORKERS = 4\n",
        "CHECKPOINT_FILE = \"b0.pth.tar\"\n",
        "PIN_MEMORY = True\n",
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = True\n",
        "\n",
        "train_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=96, height=96),\n",
        "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.8),\n",
        "        A.HorizontalFlip(p=0.4),\n",
        "        A.RandomBrightnessContrast(contrast_limit=0.5, brightness_limit=0.5, p=0.2),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=0.8),\n",
        "            A.CLAHE(p=0.8),\n",
        "            A.ImageCompression(p=0.8),\n",
        "            A.RandomGamma(p=0.8),\n",
        "            A.Posterize(p=0.8),\n",
        "            A.Blur(p=0.8),\n",
        "        ], p=1.0),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=0.8),\n",
        "            A.CLAHE(p=0.8),\n",
        "            A.ImageCompression(p=0.8),\n",
        "            A.RandomGamma(p=0.8),\n",
        "            A.Posterize(p=0.8),\n",
        "            A.Blur(p=0.8),\n",
        "        ], p=1.0),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.2, border_mode=cv2.BORDER_CONSTANT),\n",
        "        A.Normalize(mean=[0.4897, 0.4897, 0.4897], std=[0.2330, 0.2330, 0.2330], max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ], keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
        ")\n",
        "\n",
        "\n",
        "val_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=96, width=96),\n",
        "        A.Normalize(mean=[0.4897, 0.4897, 0.4897], std=[0.2330, 0.2330, 0.2330], max_pixel_value=255.0),\n",
        "        ToTensorV2(),\n",
        "    ], keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sNoiTd3DYuQy"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(loader, model, optimizer, loss_fn, scaler, device):\n",
        "    losses = []\n",
        "    loop = tqdm(loader)\n",
        "    num_examples = 0\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        scores = model(data)\n",
        "        scores[targets == -1] = -1\n",
        "        loss = loss_fn(scores, targets)\n",
        "        num_examples += torch.numel(scores[targets != -1])\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Loss average over epoch: {(sum(losses)/num_examples)**0.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxXIdrAYvnU",
        "outputId": "874ff269-d4d6-4b67-9728-35471a5aad91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viGO8yLlY1lI",
        "outputId": "02992dc1-20b2-4f77-a76b-8161a4e0d179"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_ds = FacialKeypointDataset(\n",
        "    csv_file=\"/content/gdrive/MyDrive/Colab Notebooks/training_new.csv\",\n",
        "    transform=train_transforms\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True\n",
        ")\n",
        "val_ds = FacialKeypointDataset(\n",
        "    transform=val_transforms,\n",
        "    csv_file=\"/content/gdrive/MyDrive/Colab Notebooks/val_new.csv\"\n",
        ")\n",
        "val_loader=DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=False\n",
        ")\n",
        "# test_ds = FacialKeypointDataset(\n",
        "#     csv_file=\"/Users/nikhilanand/JupyterNotebooks/InterIIT2023/CVSelections/test.csv\",\n",
        "#     transform=val_transforms,\n",
        "#     train=False,\n",
        "# )\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_ds,\n",
        "#     batch_size=1,\n",
        "#     num_workers=NUM_WORKERS,\n",
        "#     pin_memory=PIN_MEMORY,\n",
        "#     shuffle=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4N4neVcZ1Hg",
        "outputId": "05bbbb3d-eb77-4ca1-8358-c8e0ca5ead22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/05/16 21:57:00 WARNING mlflow.utils.validation: Param value 'EfficientNet(\n",
            "  (_conv_stem): Conv2dStaticSamePadding(\n",
            "    3, 32, kernel_size=(3, 3), stride=(2, 2),...' (14651 characters) is truncated to 6000 characters to meet the length limit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Epoch  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on val: 51.993571304554024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:56<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 52.177165239145424\n",
            "Saving checkpoint...\n",
            "Epoch  1\n",
            "Loss on val: 49.017125975242045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:58<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 49.050640185495176\n",
            "Saving checkpoint...\n",
            "Epoch  2\n",
            "Loss on val: 37.20966510299818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 45.142964355041556\n",
            "Saving checkpoint...\n",
            "Epoch  3\n",
            "Loss on val: 34.839114251476396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:54<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 41.960121831878304\n",
            "Saving checkpoint...\n",
            "Epoch  4\n",
            "Loss on val: 31.406737092626255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 38.50914958697213\n",
            "Saving checkpoint...\n",
            "Epoch  5\n",
            "Loss on val: 23.323405567718726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 33.971181306465866\n",
            "Saving checkpoint...\n",
            "Epoch  6\n",
            "Loss on val: 20.726881388419923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 29.574870199094477\n",
            "Saving checkpoint...\n",
            "Epoch  7\n",
            "Loss on val: 19.081702078881577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 25.96631550435859\n",
            "Saving checkpoint...\n",
            "Epoch  8\n",
            "Loss on val: 18.973726133116067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 23.157715383136342\n",
            "Saving checkpoint...\n",
            "Epoch  9\n",
            "Loss on val: 18.602223559203903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 20.790774215229984\n",
            "Saving checkpoint...\n",
            "Loss on val: 18.150844707226472\n"
          ]
        }
      ],
      "source": [
        "LEARNING_RATE = 8e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial2\"):\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    model._fc = nn.Linear(1280, 30)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\":10\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0uU1VQoZ55E",
        "outputId": "0435207e-b36a-4f51-86d3-d8488d812440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/05/16 22:11:33 WARNING mlflow.utils.validation: Param value 'EfficientNet(\n",
            "  (_conv_stem): Conv2dStaticSamePadding(\n",
            "    3, 32, kernel_size=(3, 3), stride=(2, 2),...' (14651 characters) is truncated to 6000 characters to meet the length limit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Epoch  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on val: 52.02158735831046\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:56<00:00,  1.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 52.149076874336515\n",
            "Saving checkpoint...\n",
            "Epoch  1\n",
            "Loss on val: 48.52466003450478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 48.739591723689834\n",
            "Saving checkpoint...\n",
            "Epoch  2\n",
            "Loss on val: 40.666075533578734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:58<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 45.19311609120866\n",
            "Saving checkpoint...\n",
            "Epoch  3\n",
            "Loss on val: 34.41535175854999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 42.255248334200104\n",
            "Saving checkpoint...\n",
            "Epoch  4\n",
            "Loss on val: 28.580381907061973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 38.867946120983866\n",
            "Saving checkpoint...\n",
            "Epoch  5\n",
            "Loss on val: 21.955090940657062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:04<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 34.19857240045058\n",
            "Saving checkpoint...\n",
            "Epoch  6\n",
            "Loss on val: 19.23831741098052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 29.781675494694365\n",
            "Saving checkpoint...\n",
            "Epoch  7\n",
            "Loss on val: 19.15997294145724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 27.453023814470797\n",
            "Saving checkpoint...\n",
            "Epoch  8\n",
            "Loss on val: 22.029197594284373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:59<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 26.954230657256062\n",
            "Saving checkpoint...\n",
            "Epoch  9\n",
            "Loss on val: 23.60144120486168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:54<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 26.547189677937865\n",
            "Saving checkpoint...\n",
            "Loss on val: 24.015826707653908\n"
          ]
        }
      ],
      "source": [
        "LEARNING_RATE = 8e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial3\"):\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    model._fc = nn.Linear(1280, 30)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\":10\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        if(epoch>=7):\n",
        "          LEARNING_RATE = 1e-5\n",
        "          optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 8e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial4\"):\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    model._fc = nn.Linear(1280, 30)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\":20\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    for epoch in range(30):\n",
        "        if(epoch>=12):\n",
        "          LEARNING_RATE = 4e-5\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
            "100%|██████████| 20.4M/20.4M [00:00<00:00, 168MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/05/19 03:42:14 WARNING mlflow.utils.validation: Param value 'EfficientNet(\n",
            "  (_conv_stem): Conv2dStaticSamePadding(\n",
            "    3, 32, kernel_size=(3, 3), stride=(2, 2),...' (14651 characters) is truncated to 6000 characters to meet the length limit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on val: 52.02054212261701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 52.09736567541889\n",
            "Saving checkpoint...\n",
            "Epoch  1\n",
            "Loss on val: 48.73420589961318\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:56<00:00,  1.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 48.98647926723988\n",
            "Saving checkpoint...\n",
            "Epoch  2\n",
            "Loss on val: 39.90086540589632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 45.45742687044543\n",
            "Saving checkpoint...\n",
            "Epoch  3\n",
            "Loss on val: 35.28703455271773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 42.25474341829747\n",
            "Saving checkpoint...\n",
            "Epoch  4\n",
            "Loss on val: 27.024448661625755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 38.13832930648626\n",
            "Saving checkpoint...\n",
            "Epoch  5\n",
            "Loss on val: 21.139501905657358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:03<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 33.570334658596764\n",
            "Saving checkpoint...\n",
            "Epoch  6\n",
            "Loss on val: 21.22171111957808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 29.425557749117935\n",
            "Saving checkpoint...\n",
            "Epoch  7\n",
            "Loss on val: 21.054081442855264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 25.92488451869707\n",
            "Saving checkpoint...\n",
            "Epoch  8\n",
            "Loss on val: 20.011177580892802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:02<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 23.166445091313093\n",
            "Saving checkpoint...\n",
            "Epoch  9\n",
            "Loss on val: 18.99138438248682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 20.822452436449122\n",
            "Saving checkpoint...\n",
            "Epoch  10\n",
            "Loss on val: 18.4022986282466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:59<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 18.884351168546655\n",
            "Saving checkpoint...\n",
            "Epoch  11\n",
            "Loss on val: 17.655425364343493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 17.23685037322955\n",
            "Saving checkpoint...\n",
            "Epoch  12\n",
            "Loss on val: 17.182459841527184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 16.002856236136004\n",
            "Saving checkpoint...\n",
            "Epoch  13\n",
            "Loss on val: 16.317046739791532\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 14.980618256465437\n",
            "Saving checkpoint...\n",
            "Epoch  14\n",
            "Loss on val: 15.93917479607515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 14.152028636024177\n",
            "Saving checkpoint...\n",
            "Epoch  15\n",
            "Loss on val: 15.213623216554646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:02<00:00,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 13.470500462894357\n",
            "Saving checkpoint...\n",
            "Epoch  16\n",
            "Loss on val: 14.62013000005147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 12.923151476860104\n",
            "Saving checkpoint...\n",
            "Epoch  17\n",
            "Loss on val: 13.642184168233898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:58<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 12.58192400652419\n",
            "Saving checkpoint...\n",
            "Epoch  18\n",
            "Loss on val: 13.694223169330646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 12.292303483213201\n",
            "Saving checkpoint...\n",
            "Epoch  19\n",
            "Loss on val: 12.934819787526735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 12.013270365425969\n",
            "Saving checkpoint...\n",
            "Epoch  20\n",
            "Loss on val: 12.658677655061776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:03<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.889793240276449\n",
            "Saving checkpoint...\n",
            "Epoch  21\n",
            "Loss on val: 12.566313108169545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.7513533578166\n",
            "Saving checkpoint...\n",
            "Epoch  22\n",
            "Loss on val: 12.162422332575556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:58<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.608756286088804\n",
            "Saving checkpoint...\n",
            "Epoch  23\n",
            "Loss on val: 11.965133038866858\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:54<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.427823147611882\n",
            "Saving checkpoint...\n",
            "Epoch  24\n",
            "Loss on val: 11.586720141436379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.509558049674371\n",
            "Saving checkpoint...\n",
            "Epoch  25\n",
            "Loss on val: 11.653978781402987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [01:03<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.519088001135456\n",
            "Saving checkpoint...\n",
            "Epoch  26\n",
            "Loss on val: 11.697023147724906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:54<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.279460829090102\n",
            "Saving checkpoint...\n",
            "Epoch  27\n",
            "Loss on val: 11.48500845752062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.375297586148362\n",
            "Saving checkpoint...\n",
            "Epoch  28\n",
            "Loss on val: 11.454299880613029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.27605337079921\n",
            "Saving checkpoint...\n",
            "Epoch  29\n",
            "Loss on val: 11.323940255975547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.354822469340286\n",
            "Saving checkpoint...\n",
            "Loss on val: 11.265817452254359\n"
          ]
        }
      ],
      "source": [
        "LEARNING_RATE = 8e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial2\"):\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    model._fc = nn.Linear(1280, 30)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\":20\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    for epoch in range(30):\n",
        "        if(epoch>=12):\n",
        "          LEARNING_RATE = 4e-5\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"/content/b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on val: 11.150184555077525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:56<00:00,  1.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.232392324555297\n",
            "Saving checkpoint...\n",
            "Epoch  1\n",
            "Loss on val: 10.981878404455143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.177827058839688\n",
            "Saving checkpoint...\n",
            "Epoch  2\n",
            "Loss on val: 10.896000897610119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss average over epoch: 11.218126210772553\n",
            "Saving checkpoint...\n",
            "Epoch  3\n",
            "Loss on val: 10.97083370964329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 8/84 [00:05<00:48,  1.55it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-18-5c69866031e1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m      7\u001b[0m         \u001b[0mget_rmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSAVE_MODEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m<ipython-input-12-205c36a3b2ac>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, device)\u001b[0m\n",
            "\u001b[1;32m      3\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m      4\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m      7\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m   1283\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1284\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m   1286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1287\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n",
            "\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n",
            "\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n",
            "\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "LEARNING_RATE = 4e-6\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial5\"):\n",
        "    for epoch in range(10):\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
