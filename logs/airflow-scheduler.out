[2024-05-19 02:27:40 +0530] [30634] [INFO] Starting gunicorn 20.1.0
[2024-05-19 02:27:40 +0530] [30634] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-19 02:27:40 +0530] [30634] [ERROR] Retrying in 1 second.
[2024-05-19 02:27:41 +0530] [30634] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-19 02:27:41 +0530] [30634] [ERROR] Retrying in 1 second.
[2024-05-19 02:27:42 +0530] [30634] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-19 02:27:42 +0530] [30634] [ERROR] Retrying in 1 second.
[2024-05-19 02:27:43 +0530] [30634] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-19 02:27:43 +0530] [30634] [ERROR] Retrying in 1 second.
[2024-05-19 02:27:44 +0530] [30634] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-19 02:27:44 +0530][[34m2024-05-19 02:29:24,198[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-18T00:00:00+00:00[0m
[[34m2024-05-19 02:29:24,219[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-19 02:29:24,220[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:29:24,221[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:29:24,221[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-19 02:29:24,222[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='scheduled__2024-05-17T00:00:00+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-19 02:29:24,222[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:29:24,223[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:29:24,997[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:29:25,380[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:29:25,388[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:29:25,400[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-17T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:29:49,642[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=scheduled__2024-05-17T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:29:49,647[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=scheduled__2024-05-17T00:00:00+00:00, run_start_date=2024-05-18 20:59:25.436507+00:00, run_end_date=2024-05-18 20:59:49.436312+00:00, run_duration=23.999805, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-19 02:29:50,536[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.unzip_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-19 02:29:50,537[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:29:50,537[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:29:50,538[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.unzip_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-19 02:29:50,539[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='scheduled__2024-05-17T00:00:00+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-19 02:29:50,539[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:29:50,540[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:29:51,319[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:29:51,683[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:29:51,691[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:29:51,703[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data scheduled__2024-05-17T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:29:52,687[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=scheduled__2024-05-17T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:29:52,692[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=scheduled__2024-05-17T00:00:00+00:00, run_start_date=2024-05-18 20:59:51.740397+00:00, run_end_date=2024-05-18 20:59:52.516841+00:00, run_duration=0.776444, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-19 02:29:53,534[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-19 02:29:53,535[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:29:53,535[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:29:53,535[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-19 02:29:53,536[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='scheduled__2024-05-17T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-19 02:29:53,536[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:29:53,537[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:29:54,304[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:29:54,663[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:29:54,671[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:29:54,683[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-17T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:30:02,985[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=scheduled__2024-05-17T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:30:02,992[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=scheduled__2024-05-17T00:00:00+00:00, run_start_date=2024-05-18 20:59:54.720098+00:00, run_end_date=2024-05-18 21:00:02.770710+00:00, run_duration=8.050612, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-19 02:30:03,897[0m] {[34mdagrun.py:[0m545} INFO[0m - Marking run <DagRun keypoints_dag @ 2024-05-17 00:00:00+00:00: scheduled__2024-05-17T00:00:00+00:00, externally triggered: False> successful[0m
[[34m2024-05-19 02:30:03,923[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-17 00:00:00+00:00, run_id=scheduled__2024-05-17T00:00:00+00:00, run_start_date=2024-05-18 20:59:24.203844+00:00, run_end_date=2024-05-18 21:00:03.923788+00:00, run_duration=39.719944, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-17 00:00:00+00:00, data_interval_end=2024-05-18 00:00:00+00:00, dag_hash=473479d011a0d3fec520b125556f37a4[0m
[[34m2024-05-19 02:30:03,925[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-18T00:00:00+00:00[0m
[[34m2024-05-19 02:30:53,499[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:00:52.693658+00:00 [scheduled]>[0m
[[34m2024-05-19 02:30:53,500[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:30:53,500[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:30:53,500[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:00:52.693658+00:00 [scheduled]>[0m
[[34m2024-05-19 02:30:53,501[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-18T21:00:52.693658+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-19 02:30:53,501[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:00:52.693658+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:30:53,502[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:00:52.693658+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:30:54,449[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:30:54,919[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:30:54,929[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:30:54,942[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:00:52.693658+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:31:26,989[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-18T21:00:52.693658+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:31:26,994[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-18T21:00:52.693658+00:00, run_start_date=2024-05-18 21:00:54.981068+00:00, run_end_date=2024-05-18 21:01:26.772594+00:00, run_duration=31.791526, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-19 02:31:27,873[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:00:52.693658+00:00 [scheduled]>[0m
[[34m2024-05-19 02:31:27,873[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:31:27,874[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:31:27,874[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:00:52.693658+00:00 [scheduled]>[0m
[[34m2024-05-19 02:31:27,875[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='manual__2024-05-18T21:00:52.693658+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-19 02:31:27,875[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:00:52.693658+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:31:27,876[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:00:52.693658+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:31:28,638[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:31:28,996[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:31:29,004[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:31:29,016[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:00:52.693658+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:31:29,987[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=manual__2024-05-18T21:00:52.693658+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:31:29,993[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=manual__2024-05-18T21:00:52.693658+00:00, run_start_date=2024-05-18 21:01:29.054580+00:00, run_end_date=2024-05-18 21:01:29.788383+00:00, run_duration=0.733803, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-19 02:31:30,808[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:00:52.693658+00:00 [scheduled]>[0m
[[34m2024-05-19 02:31:30,809[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:31:30,809[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:31:30,809[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:00:52.693658+00:00 [scheduled]>[0m
[[34m2024-05-19 02:31:30,810[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-18T21:00:52.693658+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-19 02:31:30,811[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:00:52.693658+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:31:30,811[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:00:52.693658+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:31:31,573[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:31:31,926[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:31:31,933[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:31:31,945[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:00:52.693658+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:31:41,020[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-18T21:00:52.693658+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:31:41,026[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-18T21:00:52.693658+00:00, run_start_date=2024-05-18 21:01:31.983654+00:00, run_end_date=2024-05-18 21:01:40.785643+00:00, run_duration=8.801989, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-19 02:31:41,971[0m] {[34mdagrun.py:[0m545} INFO[0m - Marking run <DagRun keypoints_dag @ 2024-05-18 21:00:52.693658+00:00: manual__2024-05-18T21:00:52.693658+00:00, externally triggered: True> successful[0m
[[34m2024-05-19 02:31:41,972[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-18 21:00:52.693658+00:00, run_id=manual__2024-05-18T21:00:52.693658+00:00, run_start_date=2024-05-18 21:00:53.483686+00:00, run_end_date=2024-05-18 21:01:41.972051+00:00, run_duration=48.488365, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-17 00:00:00+00:00, data_interval_end=2024-05-18 00:00:00+00:00, dag_hash=8ac10d0c2089adc9d4233de09dea4e92[0m
[[34m2024-05-19 02:31:41,973[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-18T00:00:00+00:00[0m
[[34m2024-05-19 02:32:41,262[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-19 02:32:49,077[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:02:48.169972+00:00 [scheduled]>[0m
[[34m2024-05-19 02:32:49,078[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:32:49,078[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:32:49,078[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:02:48.169972+00:00 [scheduled]>[0m
[[34m2024-05-19 02:32:49,079[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-18T21:02:48.169972+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-19 02:32:49,079[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:02:48.169972+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:32:49,080[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:02:48.169972+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:32:49,940[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:32:50,301[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:32:50,310[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:32:50,322[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:02:48.169972+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:33:16,123[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-18T21:02:48.169972+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:33:16,128[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-18T21:02:48.169972+00:00, run_start_date=2024-05-18 21:02:50.359355+00:00, run_end_date=2024-05-18 21:03:15.930895+00:00, run_duration=25.57154, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-19 02:33:17,023[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:02:48.169972+00:00 [scheduled]>[0m
[[34m2024-05-19 02:33:17,024[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:33:17,024[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:33:17,025[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:02:48.169972+00:00 [scheduled]>[0m
[[34m2024-05-19 02:33:17,025[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='manual__2024-05-18T21:02:48.169972+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-19 02:33:17,026[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:02:48.169972+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:33:17,026[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:02:48.169972+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:33:17,799[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:33:18,149[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:33:18,156[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:33:18,168[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:02:48.169972+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:33:19,134[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=manual__2024-05-18T21:02:48.169972+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:33:19,140[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=manual__2024-05-18T21:02:48.169972+00:00, run_start_date=2024-05-18 21:03:18.206612+00:00, run_end_date=2024-05-18 21:03:18.953372+00:00, run_duration=0.74676, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-19 02:33:20,010[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:02:48.169972+00:00 [scheduled]>[0m
[[34m2024-05-19 02:33:20,011[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:33:20,011[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:33:20,011[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:02:48.169972+00:00 [scheduled]>[0m
[[34m2024-05-19 02:33:20,012[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-18T21:02:48.169972+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-19 02:33:20,012[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:02:48.169972+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:33:20,013[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:02:48.169972+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:33:20,789[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:33:21,140[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:33:21,147[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:33:21,159[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:02:48.169972+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:33:21,483[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-18T21:02:48.169972+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:33:21,488[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-18T21:02:48.169972+00:00, run_start_date=2024-05-18 21:03:21.196424+00:00, run_end_date=2024-05-18 21:03:21.272430+00:00, run_duration=0.076006, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-19 02:33:22,459[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-18 21:02:48.169972+00:00: manual__2024-05-18T21:02:48.169972+00:00, externally triggered: True> failed[0m
[[34m2024-05-19 02:33:22,459[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-18 21:02:48.169972+00:00, run_id=manual__2024-05-18T21:02:48.169972+00:00, run_start_date=2024-05-18 21:02:49.060995+00:00, run_end_date=2024-05-18 21:03:22.459537+00:00, run_duration=33.398542, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-17 00:00:00+00:00, data_interval_end=2024-05-18 00:00:00+00:00, dag_hash=5d7d011f98c4ba0d1ded6f56d4f2610b[0m
[[34m2024-05-19 02:33:22,461[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-18T00:00:00+00:00[0m
[[34m2024-05-19 02:34:04,181[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:04:03.051208+00:00 [scheduled]>[0m
[[34m2024-05-19 02:34:04,182[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:34:04,182[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:34:04,183[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:04:03.051208+00:00 [scheduled]>[0m
[[34m2024-05-19 02:34:04,184[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-18T21:04:03.051208+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-19 02:34:04,184[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:04:03.051208+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:04,185[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:04:03.051208+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:05,072[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:34:05,443[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:05,451[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:05,462[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:04:03.051208+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:34:28,518[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-18T21:04:03.051208+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:34:28,525[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-18T21:04:03.051208+00:00, run_start_date=2024-05-18 21:04:05.499654+00:00, run_end_date=2024-05-18 21:04:28.288184+00:00, run_duration=22.78853, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-19 02:34:29,557[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 2 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:04:07.629303+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:04:03.051208+00:00 [scheduled]>[0m
[[34m2024-05-19 02:34:29,557[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2024-05-19 02:34:29,558[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:34:29,558[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 1/16 running and queued tasks[0m
[[34m2024-05-19 02:34:29,558[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:04:07.629303+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:04:03.051208+00:00 [scheduled]>[0m
[[34m2024-05-19 02:34:29,559[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-18T21:04:07.629303+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-19 02:34:29,560[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:04:07.629303+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:29,560[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='manual__2024-05-18T21:04:03.051208+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-19 02:34:29,560[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:04:03.051208+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:29,561[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-18T21:04:07.629303+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:30,329[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:34:30,692[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:30,700[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:30,751[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-18T21:04:07.629303+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:34:55,422[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:04:03.051208+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:56,197[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:34:56,550[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:56,558[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:56,569[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:04:03.051208+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:34:57,569[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-18T21:04:07.629303+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:34:57,570[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=manual__2024-05-18T21:04:03.051208+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:34:57,576[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=manual__2024-05-18T21:04:03.051208+00:00, run_start_date=2024-05-18 21:04:56.608654+00:00, run_end_date=2024-05-18 21:04:57.356542+00:00, run_duration=0.747888, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-19 02:34:57,577[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-18T21:04:07.629303+00:00, run_start_date=2024-05-18 21:04:30.791429+00:00, run_end_date=2024-05-18 21:04:55.200266+00:00, run_duration=24.408837, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-19 02:34:58,412[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 2 tasks up for execution:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:04:07.629303+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:04:03.051208+00:00 [scheduled]>[0m
[[34m2024-05-19 02:34:58,413[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2024-05-19 02:34:58,413[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:34:58,413[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 1/16 running and queued tasks[0m
[[34m2024-05-19 02:34:58,413[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:04:07.629303+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:04:03.051208+00:00 [scheduled]>[0m
[[34m2024-05-19 02:34:58,414[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='manual__2024-05-18T21:04:07.629303+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-19 02:34:58,415[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:04:07.629303+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:58,415[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-18T21:04:03.051208+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-19 02:34:58,415[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:04:03.051208+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:58,416[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-18T21:04:07.629303+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:34:59,182[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:34:59,533[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:59,541[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:34:59,553[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data manual__2024-05-18T21:04:07.629303+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:35:00,499[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:04:03.051208+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:35:01,279[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:35:01,631[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:35:01,639[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:35:01,651[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:04:03.051208+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:35:09,984[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=manual__2024-05-18T21:04:07.629303+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:35:09,985[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-18T21:04:03.051208+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:35:09,991[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-18T21:04:03.051208+00:00, run_start_date=2024-05-18 21:05:01.689407+00:00, run_end_date=2024-05-18 21:05:09.802925+00:00, run_duration=8.113518, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-19 02:35:09,992[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=manual__2024-05-18T21:04:07.629303+00:00, run_start_date=2024-05-18 21:04:59.589479+00:00, run_end_date=2024-05-18 21:05:00.322080+00:00, run_duration=0.732601, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-19 02:35:10,878[0m] {[34mdagrun.py:[0m545} INFO[0m - Marking run <DagRun keypoints_dag @ 2024-05-18 21:04:03.051208+00:00: manual__2024-05-18T21:04:03.051208+00:00, externally triggered: True> successful[0m
[[34m2024-05-19 02:35:10,878[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-18 21:04:03.051208+00:00, run_id=manual__2024-05-18T21:04:03.051208+00:00, run_start_date=2024-05-18 21:04:04.164437+00:00, run_end_date=2024-05-18 21:05:10.878735+00:00, run_duration=66.714298, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-17 00:00:00+00:00, data_interval_end=2024-05-18 00:00:00+00:00, dag_hash=5d7d011f98c4ba0d1ded6f56d4f2610b[0m
[[34m2024-05-19 02:35:10,880[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-18T00:00:00+00:00[0m
[[34m2024-05-19 02:35:10,885[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:04:07.629303+00:00 [scheduled]>[0m
[[34m2024-05-19 02:35:10,886[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-19 02:35:10,886[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-19 02:35:10,886[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:04:07.629303+00:00 [scheduled]>[0m
[[34m2024-05-19 02:35:10,887[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-18T21:04:07.629303+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-19 02:35:10,888[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:04:07.629303+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:35:10,888[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-18T21:04:07.629303+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-19 02:35:11,651[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BDLProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-19 02:35:12,003[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-19 02:35:12,011[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-19 02:35:12,023[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-18T21:04:07.629303+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-19 02:35:21,197[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-18T21:04:07.629303+00:00 exited with status success for try_number 1[0m
[[34m2024-05-19 02:35:21,203[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-18T21:04:07.629303+00:00, run_start_date=2024-05-18 21:05:12.061524+00:00, run_end_date=2024-05-18 21:05:20.951908+00:00, run_duration=8.890384, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-19 02:35:22,086[0m] {[34mdagrun.py:[0m545} INFO[0m - Marking run <DagRun keypoints_dag @ 2024-05-18 21:04:07.629303+00:00: manual__2024-05-18T21:04:07.629303+00:00, externally triggered: True> successful[0m
[[34m2024-05-19 02:35:22,086[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-18 21:04:07.629303+00:00, run_id=manual__2024-05-18T21:04:07.629303+00:00, run_start_date=2024-05-18 21:04:29.538415+00:00, run_end_date=2024-05-18 21:05:22.086788+00:00, run_duration=52.548373, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-17 00:00:00+00:00, data_interval_end=2024-05-18 00:00:00+00:00, dag_hash=5d7d011f98c4ba0d1ded6f56d4f2610b[0m
[[34m2024-05-19 02:35:22,088[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-18T00:00:00+00:00[0m
